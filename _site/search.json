[
  {
    "objectID": "timeline.html#past-achievements",
    "href": "timeline.html#past-achievements",
    "title": "My Timeline",
    "section": " Past Achievements",
    "text": "Past Achievements\n\n2023\n\n\nQualified as an Associate Actuary (AIAA).\n\nStudied and passed Asset Liability Management and Communication, Modelling and Professionalism, thus completing the Actuary Program while working at EBM.\n\n\n\n\nGraduated with Bachelor of Science (Actuarial Science) (Honours).\n\n78% Course weighted average.\nCompleted Data Analytics Principles and Actuarial Control Cycle subjects as part of the Actuaries Institute Actuary Program.\nCompleted my honours dissertation (graded 82%): Comparing Stochastic and Constant Volatility Returns Distributions using the Heston Model.\n\n\n\n\n2022\n\n\nStarted my first graduate job at EBM Insurance & Risk.\n\n\n\n2021\n\n\nGraduated with Bachelor of Science (Actuarial Science) with Distinction.\n\n85% Course weighted average.\nObtained all Actuaries Institute Foundation Program exemptions.\nRecipient of the Curtin Excellence Scholarship."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Gender Classification using PyTorch\n\n\n\nGender Classification\n\n\nNeural Networks\n\n\nPython\n\n\nPyTorch\n\n\n\nClassification of faces into genders using a convolutional neural network with residual layers.\n\n\n\nHarry Zhong\n\n\nMay 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of My Spotify Data\n\n\n\nK-means\n\n\nR\n\n\n\nCategorisation of my Spotify listening history using k-means clustering.\n\n\n\nHarry Zhong\n\n\nMar 23, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/gender-nn/index.html",
    "href": "projects/gender-nn/index.html",
    "title": "Gender Classification using PyTorch",
    "section": "",
    "text": "Neural networks are cool, they can take complex tasks that are usually pretty easy for humans to do and automate them, given you have sufficient training data and computing power. In this project, we will explore how to make our own neural network, and attempt to predict the gender of faces.\nTo get a basic understanding of how neural networks, I would recommend watching 3Blue1Brown’s YouTube playlist on neural networks. As neural networks are slightly more complicated than most common machine learning algorithms, I won’t go through the basics in much detail here."
  },
  {
    "objectID": "projects/gender-nn/index.html#training",
    "href": "projects/gender-nn/index.html#training",
    "title": "Gender Classification using PyTorch",
    "section": "Training",
    "text": "Training\nFor this project, we’ll require a dataset containing a large number of labelled images of faces, which as you can imagine isn’t all that common. Luckily for us, the CelebA is a publicly available labelled dataset of around 200k faces. As it’s a well known dataset, there is a function in torch that automatically downloads the required files (sometimes, usually the Google drive link is down) and creates a dataset object for the CelebA.\n\nimsize = int(128/0.8)\nbatch_size = 10\nclasses = ('Female', 'Male')\n\nfivecrop_transform = transforms.Compose([\n    transforms.Resize([imsize, imsize]),\n    transforms.Grayscale(1),\n    transforms.FiveCrop(int(imsize*0.8)),\n    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))\n])\n\ntrain_dataset = datasets.CelebA(\n    root = './',\n    split='all',\n    target_type='attr',\n    transform=fivecrop_transform,\n    download=True\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    generator=torch.Generator(device=device)\n)\n\nWe can verify the number of training images using len.\n\nlen(train_dataset)\n\n202599\n\n\nNote that the set of transformations applied to the training dataset contains FiveCrop in addition to the standard resize and grayscale transformations, FiveCrop makes 5 cropped versions of each image (who would have guessed), one for each corner plus centered. This improves model performance and reduces overfitting to the training dataset. However, this also increases the computational resources required to train the model on this dataset by a factor of 5.\n\n\n\n\n\n\nNote\n\n\n\nThere is also a TenCrop function which applies the transformations from FiveCrop, plus a vertical flip. I would have liked to use TenCrop, but my old MacBook did not agree with that decision.\n\n\nWe can then access a few sample training images and their labels as we did previously.\n\ntrain_data = iter(train_loader)\ntrain_images, train_labels = next(train_data)\n\n# Index of Male label, as CelebA contains multiple labels.\nfactor = functions.attributes.index('Male')\n\nfunctions.imshow(torchvision.utils.make_grid(\n    torch.cat((\n        train_images[0],\n        train_images[1],\n        train_images[2]\n    )),\n    nrow=5\n))\n\nfor i in range(3):\n    print(classes[train_labels[:, factor][i]])\n\n\n\n\n\n\n\n\nMale\nMale\nFemale"
  },
  {
    "objectID": "projects/gender-nn/index.html#testing",
    "href": "projects/gender-nn/index.html#testing",
    "title": "Gender Classification using PyTorch",
    "section": "Testing",
    "text": "Testing\nNext, we need a dataset to test the performance of our model on unseen data. The simple option would be to split CelebA into train and test partitions. However, I found achieving high test accuracy under this setup to be fairly simple, and resulted in poor performance on other image datasets.\nThus, we’ll use a Kaggle dataset of AI generated faces as the test dataset, which I found required a significantly more complicated model to achieve high accuracy in, but produced models with better performance when given a random selection of my own images.\n\ntest_transform = transforms.Compose([\n    transforms.Resize([int(imsize*0.8), int(imsize*0.8)]),\n    transforms.Grayscale(1),\n    transforms.ToTensor()\n])\n\ntest_dataset = datasets.ImageFolder(\n    root='ThisPersonDoesNotExist_resize/',\n    transform=test_transform\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    generator=torch.Generator(device=device)\n)\n\nOnce again, we can get the number of images in the test dataset.\n\nlen(test_dataset)\n\n6873\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis dataset was originally the training dataset, given significantly reduced number of images compared to CelebA, it’s unsurprising the initial models did not perform well.\n\n\nWe can then show a few images from the test dataset, along with their labels.\n\ntest_data = iter(test_loader)\ntest_images, test_labels = next(test_data)\n\nfunctions.imshow(torchvision.utils.make_grid(test_images, nrow=5))\n\nfor i in range(batch_size):\n    print(classes[test_labels[i]])\n\n\n\n\n\n\n\n\nFemale\nFemale\nMale\nFemale\nFemale\nFemale\nFemale\nFemale\nFemale\nMale"
  },
  {
    "objectID": "projects/spotify-analysis/index.html",
    "href": "projects/spotify-analysis/index.html",
    "title": "Analysis of My Spotify Data",
    "section": "",
    "text": "The motivation for this project was:\n\nI thought it would be fun.\nThat’s it.\n\nSo, let’s get into how we can use R and Spotify’s Web API to categorise songs that we have listened to."
  },
  {
    "objectID": "projects/spotify-analysis/index.html#activity-history-data",
    "href": "projects/spotify-analysis/index.html#activity-history-data",
    "title": "Analysis of My Spotify Data",
    "section": "Activity History Data",
    "text": "Activity History Data\nOur Spotify activity history data is given as a set of .json files. We can extract the data from all the .json files into a dataframe and perform some preliminary cleaning using the code below.\n\n\nActivity History Code\nfiles &lt;- list.files(\n  \"data/full_history_data\", \n  pattern = \"*.json\", \n  full.names = TRUE\n)\n\nfull_streaming_history &lt;- foreach(\n  file = files, \n  .packages = c(\"jsonlite\"),\n  .combine = rbind\n) %do% {\n  fromJSON(file, flatten = TRUE)\n} %&gt;%\n  rename(\n    track_name = \"master_metadata_track_name\",\n    artist_name = \"master_metadata_album_artist_name\"\n  ) %&gt;%\n  mutate(\n    track_uri = gsub(\n      \"spotify:track:\", \n      \"\", \n      spotify_track_uri\n    ),\n    month = ts %&gt;%\n      substring(1, 7) %&gt;%\n      paste0(\"-01\") %&gt;%\n      ymd()\n  ) %&gt;%\n  select(\n    -spotify_track_uri,\n    -username,\n    -platform,\n    -ip_addr_decrypted\n  ) %&gt;%\n  filter(month &gt;= ymd(\"2019-04-01\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nts\nms_played\nconn_country\nuser_agent_decrypted\ntrack_name\nartist_name\nmaster_metadata_album_album_name\nepisode_name\nepisode_show_name\nspotify_episode_uri\nreason_start\nreason_end\nshuffle\nskipped\noffline\noffline_timestamp\nincognito_mode\ntrack_uri\nmonth\n\n\n\n\n2024-02-08T23:57:04Z\n2072\nAU\nunknown\nKiss Me More (feat. SZA)\nDoja Cat\nPlanet Her\nNA\nNA\nNA\ntrackdone\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436622\nFALSE\n3DarAbFujv6eYNliUTyqtz\n2024-02-01\n\n\n2024-02-08T23:57:01Z\n219724\nAU\nunknown\nvampire\nOlivia Rodrigo\nGUTS\nNA\nNA\nNA\nfwdbtn\ntrackdone\nFALSE\nFALSE\nFALSE\n1707436403\nFALSE\n1kuGVB7EU95pJObxwvfwKS\n2024-02-01\n\n\n2024-02-08T23:53:23Z\n2284\nAU\nunknown\nJudas\nLady Gaga\nBorn This Way\nNA\nNA\nNA\nfwdbtn\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436400\nFALSE\n7F25roCtYi55JouckaayPC\n2024-02-01\n\n\n2024-02-08T23:53:20Z\n1721\nAU\nunknown\nHeads Will Roll\nYeah Yeah Yeahs\nIt’s Blitz!\nNA\nNA\nNA\nfwdbtn\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436398\nFALSE\n2WRFD9WczJ975X2K1Y9YVs\n2024-02-01\n\n\n2024-02-08T23:53:18Z\n4018\nAU\nunknown\nWhat You Waiting For?\nGwen Stefani\nLove Angel Music Baby\nNA\nNA\nNA\nfwdbtn\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436394\nFALSE\n0ny5zITdmyNwyTPVzRGscU\n2024-02-01\n\n\n2024-02-08T23:53:14Z\n2404\nAU\nunknown\nSpace Song\nBeach House\nDepression Cherry\nNA\nNA\nNA\nfwdbtn\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436392\nFALSE\n3CLhX1JkJZ4s5umNnOqCRh\n2024-02-01\n\n\n\n\n\nFrom here, we can use the ggplot2 and shiny packages to visualise trends in my most listened to artists and tracks.\n\n\n\napp.R\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(foreach)\n\nfiles &lt;- list.files(\n  paste0(here(), \"/data/full_history_data\"), \n  pattern = \"*.json\", \n  full.names = TRUE\n)\n\nfull_streaming_history &lt;- foreach(\n  file = files, \n  .packages = c(\"jsonlite\"),\n  .combine = rbind\n) %do% {\n  fromJSON(file, flatten = TRUE)\n} %&gt;%\n  rename(\n    track_name = \"master_metadata_track_name\",\n    artist_name = \"master_metadata_album_artist_name\"\n  ) %&gt;%\n  mutate(\n    track_uri = gsub(\"spotify:track:\", \"\", spotify_track_uri),\n    month = ts %&gt;%\n      substring(1, 7) %&gt;%\n      paste0(\"-01\") %&gt;%\n      ymd()\n  ) %&gt;%\n  select(-spotify_track_uri) %&gt;%\n  filter(month &gt;= ymd(\"2019-04-01\"))\n\nmin_date &lt;- full_streaming_history %&gt;%\n  pull(month) %&gt;%\n  min()\n\nmax_date &lt;- full_streaming_history %&gt;%\n  pull(month) %&gt;%\n  max()\n\nui &lt;- fluidPage(\n  titlePanel(\"Spotify Streaming History\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"top_n\",\n        \"Top n\",\n        min = 1,\n        max = 20,\n        value = 10\n      ),\n      sliderInput(\n        \"dates\",\n        \"Streaming History Date Range\",\n        min = min_date,\n        max = max_date,\n        value = c(max_date %m-% months(6), max_date)\n      )\n    ),\n    mainPanel(\n      tabsetPanel(\n        type = \"tabs\",\n        tabPanel(\n          \"Artist History Plot\", \n          h2(textOutput(\"artist_history_title\")),\n          plotOutput(\"artist_history_plot\")\n        ),\n        tabPanel(\n          \"Track History Plot\",\n          h2(textOutput(\"track_history_title\")),\n          plotOutput(\"track_history_plot\")\n        )\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$artist_history_title &lt;- renderText({\n    top_n &lt;- input$top_n\n    \n    paste0(\n      \"Proportion of Hours Listened: Top \",\n      top_n,\n      \" Artists\"\n    )\n  })\n  \n  output$artist_history_plot &lt;- renderPlot({\n    top_artists &lt;- data.frame(\n      month = full_streaming_history %&gt;%\n        select(month) %&gt;%\n        distinct()\n    ) %&gt;%\n      mutate(\n        top_artists = map(\n          month,\n          ~full_streaming_history %&gt;%\n            filter(month == .x) %&gt;%\n            group_by(artist_name) %&gt;%\n            summarise(time = sum(ms_played)) %&gt;%\n            slice_max(time, n = as.numeric(input$top_n)) %&gt;%\n            pull(artist_name)\n        )\n      )\n    \n    artist_summary &lt;- full_streaming_history %&gt;%\n      filter(\n        month %&gt;%\n          between(input$dates[1], input$dates[2])\n      ) %&gt;%\n      left_join(\n        top_artists,\n        by = \"month\"\n      ) %&gt;%\n      rowwise() %&gt;%\n      filter(artist_name %in% top_artists) %&gt;%\n      group_by(\n        artist_name, \n        month\n      ) %&gt;%\n      summarise(hours_listened = sum(ms_played/(1000*60)))\n    \n    ggplot(artist_summary, aes(x = month, y = hours_listened, fill = artist_name, label = artist_name)) +\n      xlab(\"Date\") +\n      ylab(\"Proportion\") +\n      geom_bar(position = \"fill\", stat = \"identity\") +\n      geom_text(size = 3, position = position_fill(vjust = 0.5)) +\n      theme(legend.position = \"none\")\n  })\n  \n  output$track_history_title &lt;- renderText({\n    top_n &lt;- input$top_n\n    \n    paste0(\n      \"Proportion of Hours Listened: Top \",\n      top_n,\n      \" Tracks\"\n    )\n  })\n  \n  output$track_history_plot &lt;- renderPlot({\n    top_tracks &lt;- data.frame(\n      month = full_streaming_history %&gt;%\n        select(month) %&gt;%\n        distinct()\n    ) %&gt;%\n      mutate(\n        top_tracks = map(\n          month,\n          ~full_streaming_history %&gt;%\n            filter(month == .x) %&gt;%\n            mutate(track_artist_name = paste(track_name, artist_name, sep = \"\\n\")) %&gt;%\n            group_by(track_artist_name) %&gt;%\n            summarise(time = sum(ms_played)) %&gt;%\n            slice_max(time, n = as.numeric(input$top_n)) %&gt;%\n            pull(track_artist_name)\n        )\n      )\n    \n    track_summary &lt;- full_streaming_history %&gt;%\n      filter(\n        month %&gt;%\n          between(input$dates[1], input$dates[2])\n      ) %&gt;%\n      mutate(track_artist_name = paste(track_name, artist_name, sep = \"\\n\")) %&gt;%\n      left_join(\n        top_tracks,\n        by = \"month\"\n      ) %&gt;%\n      rowwise() %&gt;%\n      filter(track_artist_name %in% top_tracks) %&gt;%\n      group_by(\n        track_artist_name, \n        month\n      ) %&gt;%\n      summarise(hours_listened = sum(ms_played/(1000*60)))\n    \n    ggplot(track_summary, aes(x = month, y = hours_listened, fill = track_artist_name, label = track_artist_name)) +\n      xlab(\"Date\") +\n      ylab(\"Proportion\") +\n      geom_bar(position = \"fill\", stat = \"identity\") +\n      geom_text(size = 3, position = position_fill(vjust = 0.5)) +\n      theme(legend.position = \"none\")\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "projects/spotify-analysis/index.html#track-feature-data",
    "href": "projects/spotify-analysis/index.html#track-feature-data",
    "title": "Analysis of My Spotify Data",
    "section": "Track Feature Data",
    "text": "Track Feature Data\nNext, we’ll need to use Spotify’s Web API to obtain track features, which requires the track ID of each track we’re interested in. Fortunately, since we requested our full activity history, this data is included as a column.\n\n\n\n\n\n\nNote\n\n\n\nOn the topic of Spotify’s Web API, it’s interesting to note that it also includes genres. However, genres are linked to artists, not tracks, which makes this feature less noteworthy compared to track features.\n\n\nWe can use the httr and jsonlite packages to create a function that takes a Spotify track ID and returns its track features.\n\nget_audio_features &lt;- function(track_id) {\n  url = paste0(\"https://api.spotify.com/v1/audio-features/\", track_id)\n  response &lt;- GET(\n    url,\n    add_headers(Authorization = paste(\"Bearer\", spotify_token))\n  )\n  data &lt;- fromJSON(\n    content(\n      response, \n      \"text\", \n      encoding = \"UTF-8\"\n    )\n  )\n  return(data)\n}\n\nGiven the large number of track IDs, using this function on all tracks in our dataset is a long and painful process, where we will get rate limited many times by Spotify. Conveniently, I have a local file containing all of our tracks and their associated track features, which I will load in.\nThe description for each feature can be found in Spotify’s documentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\ntrack_uri\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\ntime_signature\n\n\n\n\nPsychedelic Switch\nCarly Rae Jepsen\n7zy2kNoeD72x2NEDaAsJOX\n0.681\n0.805\n4\n-6.676\n1\n0.0480\n0.00182\n1.58e-02\n0.341\n0.304\n127.907\n272035\n4\n\n\nSick Feeling\nboy pablo\n7zxLkZbUxITHabPzGN8Xgc\n0.415\n0.504\n9\n-10.003\n1\n0.0318\n0.02200\n3.80e-06\n0.363\n0.401\n165.860\n155714\n4\n\n\nYou Get Me So High\nThe Neighbourhood\n7zwn1eykZtZ5LODrf7c0tS\n0.551\n0.881\n7\n-6.099\n0\n0.0542\n0.18600\n7.91e-02\n0.152\n0.387\n88.036\n153000\n4\n\n\nNo Different\nEpik High\n7ztlf9mCrjoLXAYYf0LCYx\n0.732\n0.600\n1\n-6.127\n1\n0.0535\n0.03070\n8.25e-05\n0.137\n0.238\n131.912\n200362\n4\n\n\nSorry\nThe Rose\n7zmrZMinkTMJ2kZgM9Kqgp\n0.388\n0.642\n10\n-4.659\n1\n0.0337\n0.47100\n0.00e+00\n0.306\n0.402\n173.610\n215477\n4\n\n\nLivin It Up (with Post Malone & A$AP Rocky)\nYoung Thug\n7zjEyeBsaw9gV0jofJLfOM\n0.767\n0.313\n7\n-12.059\n1\n0.0798\n0.83800\n0.00e+00\n0.105\n0.765\n82.582\n210907\n4\n\n\n\n\n\nWe can then remove discrete track features and scale the remaining features so that the clusters are not affected by the difference in magnitude of different features.\n\nfeature_matrix &lt;- full_track_features %&gt;%\n  mutate(track_artist = paste(track_name, artist_name, sep = \" - \")) %&gt;%\n  select(\n    -track_name, \n    -artist_name\n  ) %&gt;%\n  column_to_rownames(var = \"track_artist\") %&gt;%\n  select(\n    -track_uri,\n    -key,\n    -mode,\n    -time_signature\n  ) %&gt;%\n  scale()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\nPsychedelic Switch - Carly Rae Jepsen\n0.5034878\n1.3400006\n0.4206709\n-0.3325940\n-1.3132909\n-0.3305910\n1.3463448\n-0.5950222\n0.3365591\n1.3459625\n\n\nSick Feeling - boy pablo\n-1.2632525\n-0.0712252\n-0.3296591\n-0.5474916\n-1.2506975\n-0.3915509\n1.5205894\n-0.1757695\n1.6071509\n-0.7309358\n\n\nYou Get Me So High - The Neighbourhood\n-0.3599567\n1.6963234\n0.5508003\n-0.2503492\n-0.7420099\n-0.0863063\n-0.1505748\n-0.2362802\n-0.9982436\n-0.7793940\n\n\nNo Different - Epik High\n0.8422237\n0.3788667\n0.5444855\n-0.2596349\n-1.2237122\n-0.3912473\n-0.2693779\n-0.8802870\n0.4706386\n0.0662492\n\n\nSorry - The Rose\n-1.4425833\n0.5757820\n0.8755599\n-0.5222876\n0.1419900\n-0.3915657\n1.0691375\n-0.1714473\n1.8666057\n0.3361258\n\n\nLivin It Up (with Post Malone & A$AP Rocky) - Young Thug\n1.0746895\n-0.9667206\n-0.7933436\n0.0892422\n1.2803337\n-0.3915657\n-0.5228246\n1.3975087\n-1.1808328\n0.2545290"
  },
  {
    "objectID": "projects/spotify-analysis/index.html#what-is-k-means",
    "href": "projects/spotify-analysis/index.html#what-is-k-means",
    "title": "Analysis of My Spotify Data",
    "section": "What is K-means?",
    "text": "What is K-means?\nThe k-means algorithm basically goes:\n\nChoose \\(k\\) random points within the domain of your factors.\nCreate \\(k\\) clusters by assigning each observation to its nearest point, which is now referred to as a mean.\nThe centroid of each cluster then becomes the new mean.\nRepeat until convergence.\n\nThe obvious question is: how do we determine the value of \\(k\\) for a given set of factors? One method would be to use something called a silhouette value. We can understand the silhouette value by considering a group of clustered data points, shown below.\n\n\nChart Code\nset.seed(2687)\n\nrand_data &lt;- data.frame(\n  x = rnorm(50),\n  y = rnorm(50)\n)\n\nkm &lt;- kmeans(rand_data, 3)\n\nfviz_cluster(\n  km,\n  data = rand_data,\n  geom = \"point\"\n)\n\n\n\n\n\n\n\n\n\nWe’ll let \\(s_i\\) be the silhouette value for point \\(i\\) which belongs to cluster \\(C_I\\), then\n\\[\n\\begin{split}\ns_i&=\\frac{b_i-a_i}{\\text{max}(a_i,b_i)},\\text{ if }|C_I|&gt;1,\\\\\ns_i&=0,\\text{ if }|C_I|=0,\n\\end{split}\n\\]\nwhere\n\\[\n\\begin{split}\na_i&=\\frac{1}{|C_I|-1}\\sum_{j\\in C_I,i\\neq j}\\text{d}(i,j),\\\\\nb_i&=\\text{min}\\frac{1}{|C_J|}\\sum_{j\\in C_J}\\text{d}(i,j),\\text{ where }J\\neq I.\n\\end{split}\n\\]\nSimply put, \\(a_i\\) is the average of some measure of distance between point \\(i\\) and every other point in cluster \\(C_I\\) besides itself, and \\(b_i\\) is the minimum average of some measure of distance between \\(i\\) and every other point in some other cluster \\(C_J\\). The cluster \\(C_J\\), used to determine \\(b_i\\), is sometimes referred to as the neighboring cluster of point \\(i\\) as it is the next closest cluster after \\(C_I\\).\nThus, given the definition of \\(s_i\\), higher values of \\(s_i\\) indicate a better fit of a point \\(i\\) in its cluster \\(C_I\\).\nFollowing the definition of a silhouette value for a single point, the clustering performance of the entire dataset is calculated via the average silhouette value of all points.\nThus, we can determine the optimal number of clusters by:\n\nRunning the k-means algorithm using \\(n\\) clusters.\nEvaluating the average silhouette value.\nRepeat for a reasonable range of \\(n\\).\nRanking \\(n\\) by maximum average silhouette value.\n\nConveniently, the fviz_nbclust function takes care of this process for us."
  },
  {
    "objectID": "projects/spotify-analysis/index.html#feature-selection",
    "href": "projects/spotify-analysis/index.html#feature-selection",
    "title": "Analysis of My Spotify Data",
    "section": "Feature selection",
    "text": "Feature selection\nThe next problem is finding the optimal features to include our model. Generally speaking, an easy way to determine the relevant features to include in a model is to use domain knowledge. However, we do not have this luxury as I know nothing about audio engineering. So, we will do it the hard way, by trying every combination of features and selecting the ones with the best performance.\nTo do this, we can use a function I wrote that does the following:\n\nTakes inputs n, data, nstart, itermax. Where n is is the number of factors to consider, and data is the feature matrix previously generated. The nstart and itermax inputs are values passed on to the kmeans function.\nFinds all combinations of n factors within data.\nFor each combination, determine the optimal number of clusters using using average silhouette value, and fit k-means clusters.\nRecords performance metrics and cluster plot.\n\nThe function then outputs a dataframe that contains a row for each combination of n factors.\n\n\nFunction Code\nkmeans_select_features &lt;- function(n, data, nstart, itermax) {\n  comb_n &lt;- data %&gt;%\n    colnames() %&gt;%\n    combn(n, simplify = FALSE)\n  \n  old_cols &lt;- seq(1, n)\n  new_cols &lt;- paste0(\"factor_\", seq(1, n))\n  \n  new_cols_sym &lt;- syms(new_cols)\n  \n  factor_combinations_n &lt;- do.call(rbind.data.frame, comb_n) %&gt;%\n    rename_with(~new_cols, all_of(old_cols)) %&gt;%\n    mutate(factors = pmap(list(!!!new_cols_sym), c)) %&gt;%\n    mutate(n_factors = n) %&gt;%\n    select(-(!!new_cols)) %&gt;%\n    mutate(data = map(factors,\n                      ~data %&gt;%\n                        as.data.frame() %&gt;%\n                        select(all_of(.x)))) %&gt;%\n    mutate(n_clusters = map(data,\n                            ~fviz_nbclust(.x, \n                                          kmeans, \n                                          nstart = nstart, \n                                          iter.max = itermax)[[\"data\"]] %&gt;%\n                              slice(which.max(y)) %&gt;%\n                              select(clusters) %&gt;%\n                              as.numeric(),\n                            .progress = paste(\"Finding optimal n_clusters:\", \n                                              n, \n                                              \"factors\")) %&gt;%\n             as.numeric()) %&gt;% \n    mutate(km = map2(data,\n                     n_clusters,\n                     ~kmeans(.x, \n                             .y, \n                             nstart = nstart, \n                             iter.max = itermax,\n                             algorithm = \"MacQueen\"),\n                     .progress = paste(\"Calculating kmeans:\", \n                                       n, \n                                       \"factors\"))) %&gt;%\n    mutate(total_withinss = map(km,\n                                ~.x$tot.withinss) %&gt;%\n             as.numeric(),\n           bsstssRatio = map(km,\n                             ~.x$betweenss/.x$totss) %&gt;%\n             as.numeric()) %&gt;%\n    mutate(km_plot = map2(km,\n                          data,\n                          ~fviz_cluster(.x,\n                                        data = .y,\n                                        geom = \"point\",\n                                        ellipse.type = \"convex\"))) %&gt;%\n    arrange(desc(bsstssRatio))  \n  \n  return(factor_combinations_n)\n}\n\n\nThe function can then be used on all values of n, from 2 to the total number of factors. As this process is computationally intensive, and most R packages do not support multi-threading, we can use the foreach and doParallel packages to write a multi-threaded for loop which utilises all cores of the local computer. If we paid for all our CPU cores, we might as well use them right?\n\nset.seed(2687)\n\ncl &lt;- makeCluster(detectCores())\nregisterDoParallel(cl)\n\nkmeans_nfact &lt;- foreach(n = seq(2, ncol(feature_matrix)),\n                        .packages = c(\n                          \"tidyverse\",\n                          \"cluster\",\n                          \"factoextra\",\n                          \"rlang\"\n                        ),\n                        .combine = bind_rows) %dopar% {\n                          kmeans_select_features(n, feature_matrix, 25, 1000)\n                        } %&gt;% \n  arrange(desc(bsstssRatio))\n\nstopCluster(cl)\n\nThis results in a dataframe containing all possible combinations of factors for our dataset, and their k-means clustering results and performance, based on a value of \\(k\\) determined by average silhouette value."
  },
  {
    "objectID": "projects/spotify-analysis/index.html#cluster-playlists",
    "href": "projects/spotify-analysis/index.html#cluster-playlists",
    "title": "Analysis of My Spotify Data",
    "section": "Cluster Playlists",
    "text": "Cluster Playlists\nJust for fun, and to see if our subjective interpretation of grouping songs together aligns at all with k-means and Spotify’s API, we can calculate the top 5 tracks by hours listened for each cluster in the highest performing k-means result.\n\n\nTop 5 Tracks Code\ncluster_top_tracks &lt;- kmeans_nfact_save %&gt;%\n  slice(1) %&gt;%\n  pull(km) %&gt;%\n  pluck(1) %&gt;%\n  pluck(\"cluster\") %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"track_artist\") %&gt;%\n  rename(cluster = 2) %&gt;% \n  mutate(cluster = paste0(\"Cluster \", cluster)) %&gt;%\n  left_join(\n    full_streaming_history %&gt;%\n      select(\n        track_name,\n        artist_name,\n        ms_played\n      ) %&gt;%\n      na.omit() %&gt;%\n      group_by(\n        track_name,\n        artist_name\n      ) %&gt;%\n      summarise(\n        hours_listened = sum(ms_played/(1000*60))\n      ) %&gt;%\n      mutate(\n        track_artist = paste(\n          track_name,\n          artist_name,\n          sep = \" - \"\n        )\n      ),\n      .,\n      by = \"track_artist\"\n    ) %&gt;%\n  select(-track_artist) %&gt;%\n  group_by(cluster) %&gt;%\n  slice_max(hours_listened, n = 5) %&gt;%\n  ungroup() %&gt;%\n  select(\n    cluster,\n    track_name,\n    artist_name\n  )\n\n\n\n\n\n\n\n\n\n\n\n\ncluster\ntrack_name\nartist_name\n\n\n\n\nCluster 1\ni &lt;3 u\nboy pablo\n\n\nCluster 1\nApocalypse\nCigarettes After Sex\n\n\nCluster 1\nNothing\nBruno Major\n\n\nCluster 1\nSunsetz\nCigarettes After Sex\n\n\nCluster 1\nWonder\nADOY\n\n\nCluster 2\nMay I Ask\nLuke Chiang\n\n\nCluster 2\ndrunk\nkeshi\n\n\nCluster 2\nis your bedroom ceiling bored? (feat. Rxseboy) - Fudasca Remix\nSody\n\n\nCluster 2\ndrivers license\nOlivia Rodrigo\n\n\nCluster 2\nLocation Unknown ◐\nHONNE\n\n\nCluster 3\nchange ur mind\nSarcastic Sounds\n\n\nCluster 3\nstay4ever (feat. Mounika.)\nPowfu\n\n\nCluster 3\naffection\nBETWEEN FRIENDS\n\n\nCluster 3\nboyfriend (with Social House)\nAriana Grande\n\n\nCluster 3\nnot ur friend\nJeremy Zucker\n\n\n\n\n\nThen, we can make some playlists:\n\nCluster 1 Playlist\n\n\n\nCluster 2 Playlist\n\n\n\nCluster 3 Playlist"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Harry Zhong",
    "section": "",
    "text": "About Me\nHello and welcome to my website! I’m Harry, an Actuary (AIAA) working as an Analyst at Commonwealth Bank. I enjoy spending my time learning about data science, computers, and lifting weights.\nI made this website as a way to document various personal projects, software, and topics which I find interesting.\nIf you want to know more about me, feel free to contact me via email or connect with me on LinkedIn. Thanks for visiting!\n\n\n Experience\nCommonwealth Bank | Analyst (Risk Analytics Strategic Initiatives) | Aug 2024 - Present\nEBM Insurance & Risk | Data Analyst | Nov 2022 - Jul 2024\n\n\n Education\nActuaries Institute | Actuary Program | Jul 2023 - Oct 2023\nCurtin University | Bachelor of Science (Actuarial Science) (Honours) | Feb 2019 - Jun 2023"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "No matching items"
  }
]